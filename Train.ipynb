{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import sent_tokenize as ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"/home/jingjing/Desktop/InferSent-master/dataset/GloVe/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [\"Adult education is essential for Democracy of India. The number of grown up illiterates is great. All college and senior School students should come forward to visit villages in the summer vacation. Each one will teach one there. This will remove illiteracy and strengthen our democracy.\",\n",
    "             \"I saw a man climbing down a water pipe. He had a knife in his hand. I hit his hand with a brick. He fell down on the ground and I jumped upon him. Soon others reached there and we handed him over to the police.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Memories of childhood are unforgettable.', 'I was four years old when my grandfather died.', 'I clearly remember how everybody in the house was weeping.'], ['Today is sunny', 'We should go out for a picnic.', 'Love the weather.'], ['Adult education is essential for Democracy of India.', 'The number of grown up illiterates is great.', 'All college and senior School students should come forward to visit villages in the summer vacation.', 'Each one will teach one there.', 'This will remove illiteracy and strengthen our democracy.'], ['I saw a man climbing down a water pipe.', 'He had a knife in his hand.', 'I hit his hand with a brick.', 'He fell down on the ground and I jumped upon him.', 'Soon others reached there and we handed him over to the police.']]\n"
     ]
    }
   ],
   "source": [
    "f = Encoder()\n",
    "sentences = ['The Moon is filled wit craters.', 'It has no light of its own.', 'It gets its light from the Sun.']\n",
    "\n",
    "\n",
    "data = [['Memories of childhood are unforgettable.', 'I was four years old when my grandfather died.',\n",
    "             'I clearly remember how everybody in the house was weeping.'], ['Today is sunny', 'We should go out for a picnic.', 'Love the weather.']]\n",
    "\n",
    "for p in paragraphs:\n",
    "    data.append(ST(p))\n",
    "\n",
    "print(data)\n",
    "f.zero_grad()\n",
    "\n",
    "f.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(context_size, dim):\n",
    "    targets = np.zeros((dim, dim))\n",
    "    ctxt_sent_pos = list(range(-context_size, context_size+1))\n",
    "    ctxt_sent_pos.remove(0)\n",
    "    for ctxt in ctxt_sent_pos:\n",
    "        targets += np.eye(dim, k=ctxt)\n",
    "    targets_sum = np.sum(targets,axis=1, keepdims=True)\n",
    "    targets = targets / targets_sum\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    m = nn.Softmax(dim=-1)\n",
    "    s_pred = m(pred)\n",
    "    losses = F.binary_cross_entropy_with_logits(s_pred, target)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20(/20) words with w2v vectors\n",
      "Vocab size : 20\n",
      "loss before training:  tensor(6.0987)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    f.build_vocab(sentences, True)\n",
    "    embeddings = f.encode(sentences, len(sentences))\n",
    "    targets = make_target(1, len(sentences))\n",
    "    loss = loss_fn(embeddings, targets.float())\n",
    "    print(\"loss before training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(f.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25(/25) words with w2v vectors\n",
      "Vocab size : 25\n",
      "tensor(6.0987)\n",
      "Found 16(/16) words with w2v vectors\n",
      "Vocab size : 16\n",
      "tensor(6.0987)\n",
      "Found 44(/44) words with w2v vectors\n",
      "Vocab size : 44\n",
      "tensor(18.9293)\n",
      "Found 37(/37) words with w2v vectors\n",
      "Vocab size : 37\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.8995)\n",
      "tensor(18.9293)\n",
      "tensor(6.3744)\n",
      "tensor(6.0988)\n",
      "tensor(18.7983)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.1215)\n",
      "tensor(6.0987)\n",
      "tensor(18.9292)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n",
      "tensor(6.0987)\n",
      "tensor(6.0987)\n",
      "tensor(18.9293)\n",
      "tensor(18.9293)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for instance in data:\n",
    "        optimizer.zero_grad()\n",
    "        if epoch==0:\n",
    "            f.build_vocab(instance, True)\n",
    "        targets = make_target(1, len(instance))\n",
    "        scores = f.encode(instance, len(instance))\n",
    "        loss = loss_fn(scores, targets.float())\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20(/20) words with w2v vectors\n",
      "Vocab size : 20\n",
      "loss after training:  tensor(6.0987)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    f.build_vocab(sentences, True)\n",
    "    embeddings = f.encode(sentences, len(sentences))\n",
    "    targets = make_target(1, len(sentences))\n",
    "    loss = loss_fn(embeddings, targets.float())\n",
    "    print(\"loss after training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
