{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"/home/jingjing/Desktop/InferSent-master/dataset/GloVe/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Encoder()\n",
    "sentences = ['The Moon is filled wit craters.', 'It has no light of its own.', 'It gets its light from the Sun.']\n",
    "\n",
    "data = [['Memories of childhood are unforgettable.', 'I was four years old when my grandfather died.',\n",
    "             'I clearly remember how everybody in the house was weeping.'], ['Today is sunny', 'We should go out for a picnic.', 'Love the weather.']]\n",
    "\n",
    "f.zero_grad()\n",
    "\n",
    "f.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(context_size, dim):\n",
    "    targets = np.zeros((dim, dim))\n",
    "    ctxt_sent_pos = list(range(-context_size, context_size+1))\n",
    "    ctxt_sent_pos.remove(0)\n",
    "    for ctxt in ctxt_sent_pos:\n",
    "        targets += np.eye(3, k=ctxt)\n",
    "    targets_sum = np.sum(targets,axis=1, keepdims=True)\n",
    "    targets = targets / targets_sum\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = make_target(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    m = nn.Softmax(dim=-1)\n",
    "    s_pred = m(pred)\n",
    "    losses = F.binary_cross_entropy_with_logits(s_pred, target)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20(/20) words with w2v vectors\n",
      "Vocab size : 20\n",
      "loss before training:  tensor(0.7914)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    f.build_vocab(sentences, True)\n",
    "    embeddings = f.encode(sentences, 3)\n",
    "    loss = loss_fn(embeddings, targets.float())\n",
    "    print(\"loss before training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(f.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25(/25) words with w2v vectors\n",
      "Vocab size : 25\n",
      "tensor(0.8036)\n",
      "Found 16(/16) words with w2v vectors\n",
      "Vocab size : 16\n",
      "tensor(0.8032)\n",
      "tensor(0.7549)\n",
      "tensor(0.6783)\n",
      "tensor(0.7456)\n",
      "tensor(0.6776)\n",
      "tensor(0.7420)\n",
      "tensor(0.6776)\n",
      "tensor(0.7416)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n",
      "tensor(0.7415)\n",
      "tensor(0.6776)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for instance in data:\n",
    "        optimizer.zero_grad()\n",
    "        if epoch==0:\n",
    "            f.build_vocab(instance, True)\n",
    "        scores = f.encode(instance, 3)\n",
    "        loss = loss_fn(scores, targets.float())\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20(/20) words with w2v vectors\n",
      "Vocab size : 20\n",
      "loss after training:  tensor(0.6777)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    f.build_vocab(sentences, True)\n",
    "    embeddings = f.encode(sentences, 3)\n",
    "    loss = loss_fn(embeddings, targets.float())\n",
    "    print(\"loss after training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
