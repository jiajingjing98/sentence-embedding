{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import QTModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import sent_tokenize as ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"/home/jingjing/Desktop/InferSent-master/dataset/GloVe/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/jingjing/Desktop/big.txt', 'r')\n",
    "data = f.read()\n",
    "splat = data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = splat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Doctors lead a hard life.',\n",
    "             'Their life is very busy.',\n",
    "             'They get up early in the morning and go to the hospital.']\n",
    "data = []\n",
    "for p in paragraphs:\n",
    "    nl = ST(p)\n",
    "    if len(nl) > 1:\n",
    "        data.append(ST(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = QTModel()\n",
    "f.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(context_size, dim):\n",
    "    targets = np.zeros((dim, dim))\n",
    "    ctxt_sent_pos = list(range(-context_size, context_size+1))\n",
    "    ctxt_sent_pos.remove(0)\n",
    "    for ctxt in ctxt_sent_pos:\n",
    "        targets += np.eye(dim, k=ctxt)\n",
    "    targets_sum = np.sum(targets,axis=1, keepdims=True)\n",
    "    targets = targets / targets_sum\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    mask = 1 - torch.diag(torch.ones(pred.size(1)))\n",
    "    npred = pred * mask\n",
    "    s_pred = F.softmax(npred, -1)\n",
    "    ln = nn.BCELoss(size_average=False)\n",
    "    losses = ln(s_pred, target)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23(/23) words with w2v vectors\n",
      "Vocab size : 23\n",
      "Found 23(/23) words with w2v vectors\n",
      "Vocab size : 23\n",
      "tensor([[ 13.8343,  13.0828,  12.9733],\n",
      "        [ 13.0828,  14.3033,  13.3851],\n",
      "        [ 12.9733,  13.3851,  14.0952]])\n",
      "loss before training:  tensor(3.7054)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    f.build_vocab(sentences, True)\n",
    "    \n",
    "    embs = f(sentences, 400)\n",
    "    print(embs)\n",
    "    targets = make_target(1, len(sentences))\n",
    "    loss = loss_fn(embs, targets.float())\n",
    "    print(\"loss before training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(f.parameters(), lr=0.0005)\n",
    "nn.utils.clip_grad_norm_(f.parameters(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89715\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdata = [[i] for i in range(3)]\n",
    "for epoch in range(50):\n",
    "    i = 0\n",
    "    for instance in data[:3]:\n",
    "        optimizer.zero_grad()\n",
    "        #if epoch == 0:\n",
    "        #    f.build_vocab(instance, True)\n",
    "        targets = make_target(1, len(instance))\n",
    "        scores = f(instance, 400)\n",
    "        \n",
    "        loss = loss_fn(scores, targets.float())\n",
    "        #print(loss)\n",
    "        trdata[i].append(loss.item())\n",
    "        i += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training:  tensor(19.4918)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #f.build_vocab(sentences, True)\n",
    "    embeddings = f(sentences, 400)\n",
    "    targets = make_target(1, len(sentences))\n",
    "    loss = loss_fn(embeddings, targets.float())\n",
    "    print(\"loss after training: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ec4f8d1c9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trdata' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure(i)\n",
    "    plt.plot(trdata[i][1:])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4.1722283363342285,\n",
       " 1.9811714887619019,\n",
       " 1.7569588422775269,\n",
       " 1.5284161567687988,\n",
       " 1.4439995288848877,\n",
       " 1.4134989976882935,\n",
       " 1.4963237047195435,\n",
       " 1.5102678537368774,\n",
       " 1.4848254919052124,\n",
       " 1.5322985649108887]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f.enc_gru.weight_ih_l0 tensor([[-3.1235e-03,  1.4690e-02, -2.2814e-02,  ..., -1.8208e-02,\n",
      "          1.4399e-02, -6.8211e-03],\n",
      "        [-1.6123e-02, -2.1756e-03, -7.4266e-03,  ...,  3.0341e-02,\n",
      "          2.2185e-02,  1.9543e-02],\n",
      "        [-3.0081e-02, -1.0454e-02, -2.6432e-02,  ...,  4.5374e-03,\n",
      "          1.5086e-02,  1.4460e-02],\n",
      "        ...,\n",
      "        [-3.6253e-04, -1.7766e-02,  3.4047e-02,  ...,  1.3577e-02,\n",
      "          1.6871e-02, -2.6868e-02],\n",
      "        [-2.5595e-02,  5.4377e-03,  1.3931e-02,  ...,  1.4207e-02,\n",
      "          2.8216e-02,  9.0383e-03],\n",
      "        [-2.8259e-02, -1.4220e-02,  2.6841e-02,  ...,  1.2873e-02,\n",
      "         -7.0035e-04,  3.3505e-03]])\n",
      "f.enc_gru.weight_hh_l0 tensor([[ 2.1337e-02,  1.5936e-02,  4.5728e-03,  ...,  1.7237e-02,\n",
      "         -1.1569e-02,  5.0962e-03],\n",
      "        [ 1.6729e-02, -1.7618e-02, -3.3060e-02,  ..., -2.7745e-02,\n",
      "          2.7399e-02,  8.0824e-03],\n",
      "        [ 2.4141e-02, -1.2633e-02, -2.5680e-02,  ...,  5.5447e-03,\n",
      "          2.1274e-02, -1.1687e-02],\n",
      "        ...,\n",
      "        [-1.0813e-02, -2.6081e-02, -1.3136e-03,  ...,  2.0899e-02,\n",
      "         -1.2509e-02, -2.4670e-02],\n",
      "        [ 1.8408e-02,  7.8514e-03, -1.0270e-02,  ...,  4.8049e-03,\n",
      "         -6.2715e-04,  2.8178e-02],\n",
      "        [ 2.7475e-02,  3.8673e-03,  5.6116e-05,  ..., -1.0593e-02,\n",
      "          6.7224e-03, -3.4547e-03]])\n",
      "f.enc_gru.bias_ih_l0 tensor(1.00000e-02 *\n",
      "       [ 2.3286, -2.9982, -2.8559,  ...,  2.2537, -1.4224, -2.8515])\n",
      "f.enc_gru.bias_hh_l0 tensor([-1.1067e-02, -1.8089e-02,  2.4855e-02,  ..., -1.2177e-02,\n",
      "         6.7369e-03, -3.1100e-02])\n",
      "f.enc_gru.weight_ih_l0_reverse tensor([[-2.2287e-02,  8.9844e-03,  9.8974e-03,  ...,  2.9603e-02,\n",
      "         -1.8022e-02,  1.9998e-02],\n",
      "        [ 6.7708e-03, -4.3468e-03,  2.1662e-02,  ..., -1.6329e-03,\n",
      "         -4.3372e-03,  8.8697e-03],\n",
      "        [ 2.1955e-02, -2.1837e-02,  1.9893e-02,  ..., -1.4222e-02,\n",
      "         -1.6686e-02, -2.2487e-03],\n",
      "        ...,\n",
      "        [-1.0173e-02, -3.2461e-02,  1.7302e-02,  ...,  1.7658e-02,\n",
      "         -3.1865e-02,  1.9792e-02],\n",
      "        [ 2.5151e-02,  9.6865e-03, -5.1493e-03,  ...,  2.1672e-02,\n",
      "          1.4211e-02, -3.7976e-03],\n",
      "        [ 1.5646e-03, -6.0159e-03,  2.1232e-02,  ..., -9.7732e-04,\n",
      "          1.6416e-02,  2.5517e-02]])\n",
      "f.enc_gru.weight_hh_l0_reverse tensor([[-1.1078e-02, -2.6366e-02, -2.3974e-02,  ..., -1.0753e-02,\n",
      "         -1.1014e-02, -1.5943e-02],\n",
      "        [-2.2697e-02,  1.7997e-02,  2.5778e-02,  ...,  1.8802e-02,\n",
      "         -1.4408e-02,  1.2676e-02],\n",
      "        [ 9.2392e-03,  1.3349e-02,  1.6708e-03,  ...,  8.9539e-03,\n",
      "         -1.3746e-02, -1.9964e-02],\n",
      "        ...,\n",
      "        [ 2.6249e-02, -2.4722e-02, -6.1270e-03,  ..., -4.7372e-03,\n",
      "         -2.6831e-02, -2.4484e-02],\n",
      "        [ 1.2061e-02, -3.9378e-03,  2.1543e-02,  ...,  1.6341e-03,\n",
      "          9.2928e-03,  1.9674e-02],\n",
      "        [-2.0411e-02, -1.3427e-02, -4.6356e-04,  ..., -2.4126e-02,\n",
      "         -1.9424e-02, -6.7982e-03]])\n",
      "f.enc_gru.bias_ih_l0_reverse tensor(1.00000e-02 *\n",
      "       [-0.1778,  2.0428,  1.2867,  ...,  1.5070,  1.0357, -0.0810])\n",
      "f.enc_gru.bias_hh_l0_reverse tensor(1.00000e-02 *\n",
      "       [ 0.8400, -1.8307,  0.2823,  ...,  0.4187, -0.4699, -2.6390])\n",
      "g.enc_gru.weight_ih_l0 tensor([[-3.8957e-03,  1.0173e-02, -2.4385e-02,  ..., -1.9399e-02,\n",
      "          1.8024e-02, -1.2932e-02],\n",
      "        [-1.4040e-02,  8.6927e-05, -1.1811e-02,  ...,  2.5433e-02,\n",
      "          2.8288e-02,  2.3697e-02],\n",
      "        [-2.5495e-02, -1.2051e-02, -2.3281e-02,  ...,  1.8465e-03,\n",
      "          9.3632e-03,  9.6763e-03],\n",
      "        ...,\n",
      "        [-3.4202e-03, -2.1606e-02,  2.8503e-02,  ...,  1.6028e-02,\n",
      "          1.8552e-02, -2.3431e-02],\n",
      "        [-1.9379e-02,  5.9205e-03,  1.4417e-02,  ...,  1.1646e-02,\n",
      "          2.3721e-02,  3.5437e-03],\n",
      "        [-2.8913e-02, -1.3950e-02,  2.9562e-02,  ...,  1.1116e-02,\n",
      "         -2.5959e-03,  6.3707e-03]])\n",
      "g.enc_gru.weight_hh_l0 tensor([[ 2.7998e-02,  1.7813e-02,  4.3647e-03,  ...,  1.0156e-02,\n",
      "         -1.7825e-02,  4.1428e-03],\n",
      "        [ 1.4941e-02, -1.5558e-02, -2.8078e-02,  ..., -2.3154e-02,\n",
      "          2.5591e-02, -2.0807e-03],\n",
      "        [ 2.4443e-02, -1.8167e-02, -2.2982e-02,  ...,  8.6472e-03,\n",
      "          1.9692e-02, -9.5921e-03],\n",
      "        ...,\n",
      "        [-1.4361e-02, -2.8215e-02, -2.3457e-03,  ...,  2.2665e-02,\n",
      "         -9.1851e-03, -2.9964e-02],\n",
      "        [ 1.8511e-02,  5.2023e-03, -7.8943e-03,  ..., -1.1643e-04,\n",
      "          4.9990e-04,  2.8575e-02],\n",
      "        [ 2.8381e-02,  1.4655e-03,  3.4730e-04,  ..., -4.8118e-03,\n",
      "          9.1549e-03, -4.8059e-03]])\n",
      "g.enc_gru.bias_ih_l0 tensor(1.00000e-02 *\n",
      "       [ 1.9547, -2.7204, -2.8112,  ...,  2.4648, -1.2190, -2.6647])\n",
      "g.enc_gru.bias_hh_l0 tensor(1.00000e-02 *\n",
      "       [-1.4806, -1.5311,  2.5301,  ..., -1.2490,  0.3108, -3.0310])\n",
      "g.enc_gru.weight_ih_l0_reverse tensor([[-1.4637e-02,  5.4973e-03,  1.4400e-02,  ...,  2.4915e-02,\n",
      "         -2.7615e-02,  1.5010e-02],\n",
      "        [ 1.2237e-02, -5.8478e-03,  1.8245e-02,  ..., -4.5675e-03,\n",
      "         -4.6986e-03,  8.2215e-03],\n",
      "        [ 2.5515e-02, -2.2620e-02,  1.9856e-02,  ..., -1.6692e-02,\n",
      "         -1.5212e-02, -3.1848e-03],\n",
      "        ...,\n",
      "        [-9.0928e-03, -2.4179e-02,  1.2756e-02,  ...,  1.7697e-02,\n",
      "         -2.7288e-02,  1.6860e-02],\n",
      "        [ 2.2927e-02,  7.5245e-03, -3.2977e-03,  ...,  1.9735e-02,\n",
      "          1.7823e-02, -8.0566e-03],\n",
      "        [ 5.7144e-03, -8.3217e-03,  2.0987e-02,  ..., -3.1981e-03,\n",
      "          1.0868e-02,  2.4411e-02]])\n",
      "g.enc_gru.weight_hh_l0_reverse tensor([[-1.0370e-02, -3.3106e-02, -2.5498e-02,  ..., -2.9656e-03,\n",
      "         -7.5461e-03, -1.7743e-02],\n",
      "        [-2.1957e-02,  1.9145e-02,  2.3901e-02,  ...,  2.0983e-02,\n",
      "         -1.6214e-02,  1.5160e-02],\n",
      "        [ 5.3375e-03,  1.4542e-02,  4.5618e-03,  ...,  7.5660e-03,\n",
      "         -1.3513e-02, -1.6471e-02],\n",
      "        ...,\n",
      "        [ 2.1857e-02, -2.5995e-02, -4.9485e-03,  ..., -6.5580e-03,\n",
      "         -2.4863e-02, -2.0410e-02],\n",
      "        [ 9.7573e-03, -2.4058e-03,  2.3682e-02,  ..., -1.6713e-03,\n",
      "          1.0372e-02,  2.1309e-02],\n",
      "        [-1.6503e-02, -1.4231e-02, -5.5260e-04,  ..., -2.0305e-02,\n",
      "         -2.4020e-02, -1.1273e-02]])\n",
      "g.enc_gru.bias_ih_l0_reverse tensor(1.00000e-02 *\n",
      "       [ 0.3137,  2.3307,  1.4471,  ...,  1.7641,  1.3402,  0.1378])\n",
      "g.enc_gru.bias_hh_l0_reverse tensor([ 1.3316e-02, -1.5429e-02,  4.4273e-03,  ...,  6.1285e-03,\n",
      "        -4.1802e-03, -2.5471e-02])\n"
     ]
    }
   ],
   "source": [
    "for name, param in f.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
